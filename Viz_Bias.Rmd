---
title: "Remy's Bias Looking Glass"
author: "Nik Bear Brown"
date: "11/6/2020"
output:
  html_document: default
  word_document: default
  rmarkdown::html_vignette: default
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Remy's Bias Looking Glass  

## Chapter One - Analysizing Twitter Bias

aaa 

## Am I Biased?  Creating a Twitter Looking Glass

After tweeting in support of my favorite politician, Dwayne Elizondo Mountain Dew Herbert Camacho, several people responded calling me such names as a snollygoster, a Bieber, a lickspittle, and a ninny. That if I weren't such a tosser and twit I would support Coriolanus Snow.

Worst of all I was accussed of being biased and unfair, as I asseted they were. We got into an argument about who is more biased. Rather than argue, I decided to investigate. To accomplish this I want to build an AI looking glass which allow people to visualize bias in writing, in particular in Tweets. The AI looking glass can also be used as a mirror to look at one’s own bias.  



## Rembrandt X Einstein, Remy

By the way, my name is Rembrandt X Einstein, or Remy as my friends call me. I am an AI researcher and professor at the Smarty Pants Instutite. I build predictive models using AI all of the time but sometimes wonder if the predictions they make unfairly affect people. I also wonder what my own biases are. It is time to use math to find out. 

![Tesla MacBook Remy](https://raw.githubusercontent.com/nikbearbrown/Remys_AI_Looking_Glass/main/images/Tesla_MacBook_Remy.png)


## Can Math Detect Bias?

Let's start with a simple definition of bias. If the probability of an outcome is equal given the same inputs then this is unbiased.  For example, if two people with the same qualifications have the same probability of getting a job or having the same salary then there is not bias. Conversly, if the probability differs significantly then there is not bias. For example, if two people have the same qualifications (e.g. years experience, education, etc.) they should have the same chance of getting a job. 

In math, we call the outcome variable the target or dependent variable. Convention has a one call the target variable $y$, and in particalur it is usually called $\hat{y}$ or "y-hat" to indicate is is an estimate or prediction or $y$ as the true $y$ is rarely known. The input variables, thing like years experience, education, techincal skill, etc are called input variables or dependent variables and usually called $X$, we use a captial $X$ rather than a lower case $x$ to indicate that there can be many input variables, or a vector rather than a single scaler value.    

This leds to one possible mathematical definition of bias.

\begin{equation}  
    Pr(\hat{y}|X_{A}) = Pr(\hat{y}|X_{B})) \enspace where \enspace X_{A} = X_{B}. \tag{Eq 1}  
\end{equation}

The above reads the probability of y-hat is the same if the inputs are the same.  This is a common sense notion, that is there is no bias when the given the same input one gets the same result.

We will continue to come back to this idea the outcome being different when the relevant input is the same as we continue to refine our exploration of bias and fairness.

![Same Content with a Different Response indicates Bias](https://raw.githubusercontent.com/nikbearbrown/Remys_AI_Looking_Glass/main/images/Same_Content_Different_Response_Bias.png)


## Can We Visualize Bias?

If our notion of bias is disparate outcomes the one should be able to visualize bias. One just needs visualizations that show disparate outcomes in the data.  

![PhD Bias](https://raw.githubusercontent.com/nikbearbrown/Remys_AI_Looking_Glass/main/images/PhD_Viz-Bias.png)

The graphic above shows that those who self-indentify as White and Asian are awarded the most PhDs in the United States. This disparity has existed for decades. But is it bias?  

According to the US Census those who self-indentify as White make up 76.3 percent of the population, and those who self-indentify as Black or African American make up 13.4 percent.

A logical expectation would be that those who self-indentify as White make would be awarded around 76 percent of the PhDs, and those who self-indentify as Black or African American would be awarded around 13 percent. The expectation is called a _model_. The model essentially states that individuals of all races and ethnicities should be awarded PhDs with equal probabilites, so the proporation should reflect the number of tries. 
If one accepts this model, does the graph indicate bias? According to the [NSF Survey of Earned Doctorates](https://www.nsf.gov/statistics/srvydoctorates/) African Americans are awarded around 5-7 percent of all doctoral degrees to students who are U.S. citizens.  This means one would expect African Americans are awarded around 14 percent, but are actaully awarded around 6 percent. Further in certain fields, expecially STEM fields Blacks are vastly underrepresented among doctoral degree recipients.  

 For example, African Americans earned only 1.8 percent of all doctorates, 3.8 percent of all mathematics and statistics doctorates, 3.7 percent of all doctorates in computer science, and only 4.1 percent of all doctorates awarded in engineering disciplines. In 2017, there were more than a dozen fields—largely subfields within science, technology, engineering, and math—in which not a single doctoral degree was awarded to a black person anywhere in the United States.


## Is Bias Always Bad?

Bias just relfects the inequality of outcome. Sometimes that reflects the reality of the world. Basketball players are biased towards being tall. One reasonable model of the world would be the height helps one play basketball so this makes sense.

COVID-19 shows a clear bias with respect to age. Older people are far more likely to die from coronavirus infection.

![Age Bias COVID-19](https://raw.githubusercontent.com/nikbearbrown/Remys_AI_Looking_Glass/main/images/C19_Age_Bias.png)

In spite of a median age of 40 years old in the United States, nearly all of the COVID-19 deaths are 65 and older. There is a large age bias with respect to COVID-19 deaths. While this is, by definition, bias it is not necessarily unfair as one would expect this population to be more vulnerable based on biology. While fairness is related to bias, it is not the same thing.  



## Bias versus Fairness

Bias is when there are unequal outcomes. Fairness is when the unequal outcomes are undeserved.  For example, if a man and a woman were to apply for to a prestigious company for an artificial intelligence engineering job and the man had a PhD in Computer Science and the woman didn't graaduate college most would not consider that unfair. However, if they both had essentially the same qualifications and were both highly qualified and the man were far more likely to get the job based on the only difference being gender then most would consider that unfair. 



## Visualizing Ones Own Bias

One can explore this idea of responding to the same input differently based on something immaterial to the outcome or target of interest in order to develop algorithms and visualizations of bias, including understanding ones own bias. For example, if two politicians tweet essentially the same idea and one response depends more on whether the tweeter was liberal or conservative than the content of the tweet that would indicate bias.

We are going to call the variable that is immaterial to the outcome $p$. This would lead to a mathematical expression of fairness as follows: 


\begin{equation}  
    Pr(\hat{y}|p) = Pr(\hat{y}). \tag{Equation 2}  
\end{equation}

The equation would mean that the probability of the outcome would not be affected by the input $p$.

\begin{equation}  
    Pr(\hat{salary}|gender) = Pr(\hat{salary}). 
\end{equation}

For example, ones salary would not be affected by ones gender, but rather other input such as education, years experience, skill level, etc.

## Remy's Taco Bias

Remy and his BFF Angi eat togther but she is a vegan, and Remy is an omnivore. They came up with the following compromise for their dietary schedule.  

* Meatless Monday  
* Taco Tuesday  
* Waffle Wednesday   
* Tofu Thursday   
* Stir Fryday   
* Weekend Wheatgrass Juice   
* Weekend Whisky & Wieners    

Both Remy and Angi express their food sentiment on Twitter.  

![Remy and Angi Food Tweets](https://raw.githubusercontent.com/nikbearbrown/Remys_AI_Looking_Glass/main/images/Remy_Angi_Food_Tweets.png)
They decided to plot their Twitter food sentiment for a week. To get a baseline they also decided to plot their followers food sentiment for the same period and average those as well.

The sentiment analsyis is pretty straight forward. We classify tweets with a probability of being related to food, accept those which have a very high probability of being food related and then calculate the sentiment of those tweets by summing up the positive, negative, + neutral words and adjust by the length of the tweet.
 
![Sentiment Analysis](https://raw.githubusercontent.com/nikbearbrown/Remys_AI_Looking_Glass/main/images/Remy_Angi_Food_Tweets.png)


## Classifying Food Tweets with Naive Bayes

 We classify tweets with a probability of being related to food using the  Naive Bayes algorithm. [Naive Bayes classifiers](https://en.wikipedia.org/wiki/Naive_Bayes_classifier) are a family of simple probabilistic classifiers based on applying Bayes' theorem with strong (naive) independence assumptions between the features.

Abstractly, naive Bayes is a conditional probability model given a problem instance to be classified, represented by a vector $\mathbf{x} = (x_1, \dots, x_n)$ representing some $n$ features (independent variables), it assigns to this instance probabilities

$$p(C_k \mid x_1, \dots, x_n)\,$$

for each of $k$ possible outcomes or classes $C_k$.

The problem with the above formulation is that if the number of features $n$ is large or if a feature can take on a large number of values, then basing such a model on probability tables is infeasible.  We therefore reformulate the model to make it more tractable.  Using Bayes' theorem, the conditional probability can be decomposed as

$$p(C_k \mid \mathbf{x}) = \frac{p(C_k) \ p(\mathbf{x} \mid C_k)}{p(\mathbf{x})} \,$$

In plain English, using Bayesian probability terminology, the above equation can be written as

$$\mbox{posterior} = \frac{\mbox{prior} \times \mbox{likelihood}}{\mbox{evidence}} \,$$

In practice, there is interest only in the numerator of that fraction, because the denominator does not depend on $C$ and the values of the features $F_i$ are given, so that the denominator is effectively constant.
The numerator is equivalent to the joint probability model

$$p(C_k, x_1, \dots, x_n)\,$$

which can be rewritten as follows, using the Chain rule for repeated applications of the definition of conditional probability

$$
\begin{align}
p(C_k, x_1, \dots, x_n) & = p(x_1, \dots, x_n, C_k) \\
                        & = p(x_1 \mid x_2, \dots, x_n, C_k) p(x_2, \dots, x_n, C_k) \\
                        & = p(x_1 \mid x_2, \dots, x_n, C_k) p(x_2 \mid x_3, \dots, x_n, C_k) p(x_3, \dots, x_n, C_k) \\
                        & = \dots \\
                        & = p(x_1 \mid x_2, \dots, x_n, C_k) p(x_2 \mid x_3, \dots, x_n, C_k) \dots   p(x_{n-1} \mid x_n, C_k) p(x_n \mid C_k) p(C_k)  \\
\end{align}
$$

Now the "naive" conditional independence assumptions come into play assume that each feature $F_i$ is conditionally statistical independence|independent of every other feature $F_j$ for $j\neq i$, given the category $C$.  This means that

$$p(x_i \mid x_{i+1}, \dots ,x_{n}, C_k ) = p(x_i \mid C_k)\,$$.

Thus, the joint model can be expressed as

$$
\begin{align}
p(C_k \mid x_1, \dots, x_n) & \varpropto p(C_k, x_1, \dots, x_n) \\
                            & \varpropto p(C_k) \ p(x_1 \mid C_k) \ p(x_2\mid C_k) \ p(x_3\mid C_k) \ \cdots \\
                            & \varpropto p(C_k) \prod_{i=1}^n p(x_i \mid C_k)\,.
\end{align}
$$

This means that under the above independence assumptions, the conditional distribution over the class variable $C$ is

$$p(C_k \mid x_1, \dots, x_n) = \frac{1}{Z} p(C_k) \prod_{i=1}^n p(x_i \mid C_k)$$

where the evidence $Z = p(\mathbf{x})$ is a scaling factor dependent only on $x_1, \dots, x_n$, that is, a constant if the values of the feature variables are known.

 _Constructing a classifier from the probability model_    

The discussion so far has derived the independent feature model, that is, the naive Bayes probability model.  The naive Bayes classifier combines this model with a decision rule.  One common rule is to pick the hypothesis that is most probable; this is known as the 'maximum a posteriori' or 'MAP' decision rule.  The corresponding classifier, a Bayes classifier, is the function that assigns a class label $\hat{y} = C_k$ for some $k$ as follows

$$\hat{y} = \underset{k \in \{1, \dots, K\}}{\operatorname{argmax}} \ p(C_k) \displaystyle\prod_{i=1}^n p(x_i \mid C_k).$$

## Remy and Angi's Food Sentiment

Remy and Angi's food sentiment is as one would expect.  Remy loves tacos and waffles and is not fond of his veggies.  Angi, a vegan, loves her tofu and greens. 


![Twitter Food Sentiment](https://raw.githubusercontent.com/nikbearbrown/Remys_AI_Looking_Glass/main/images/Twitter_Food_Sentiment.png)

## Remy and Angi's Political Sentiment

Remy and Angi's food sentiment is as one would expect.  Remy loves tacos and waffles and is not fond of his veggies.  Angi, a vegan, loves her tofu and greens. 


## Introduction

_Discrimination_ is the unequal treatment of individuals of certain groups, resulting in members of one group being deprived of benefits or opportunities. Common groups that suffer discrimination include those based on age, gender, skin colour, religion, race, language, culture, marital status, or economic condition.

The unintentional unfairness that occurs when a decision has widely different outcomes for different groups is known as _ disparate impact_. As machine learning algorithms are increasingly used to determine important real-world outcomes such as loan approval, pay rates, and parole decisions, it is incumbent on the AI community to minimize unintentional discrimination. 

This tutorial discusses how _ bias_ can be introduced into the machine learning pipeline, what it means for a decision to be _ fair_, and methods to remove bias and ensure fairness.

## Where does bias come from?
There are many possible causes of bias in machine learning predictions. Here we briefly discuss three: (i) the adequacy of the data to represent different groups, (ii) bias inherent in the data, and (iii) the adequacy of the model to describe each group.

*Data adequacy.* Infrequent and specific patterns may be down-weighted by the model in the name of generalization and so minority records can be unfairly neglected. This lack of data may not just be because group membership is small; data collection methodology can exclude or disadvantage certain groups (e.g., if the data collection process is only in one language). Sometimes records are removed if they contain missing values and these may be more prevalent in some groups than others.

*Data bias.* Even if the amount of data is sufficient to represent each group, training data may reflect existing prejudices (e.g., that female workers are paid less), and this is hard to remove. Such historical unfairness in data is known as _ negative legacy_. Bias may also be introduced by more subtle means. For example, data from two locations may be collected slightly differently. If group membership varies with location this can induce biases. Finally, the choice of attributes to input into the model may induce prejudice. 

*Model adequacy.* The model architecture may describe some groups better than others. For example, a linear model may be suitable for one group but not for another.

## Definitions of fairness

A model is considered \_ fair\_ if errors are distributed similarly
across protected groups, although there are many ways to define this.
Consider taking data $\mathbf{x}$ and using a machine learning model to
compute a score $\mbox{f}\[\mathbf{x}$$\]$ that will be used to
predict a binary outcome $\hat{y}\in\{0,1\}$. Each data example
$\mathbf{x}$ is associated with a \_ protected attribute\_ $p$. In this
tutorial, we consider it to be binary $p\in\{0,1\}$. For example, it
might encode sub-populations according to gender or ethnicity.

We will refer to $p=0$ as the \_ deprived population\_ and $p=1$ as the
\_ favored population\_. Similarly we will refer to $\hat{y}=1$ as the
\_ favored outcome\_, assuming it represents the more desirable of the
two possible results.

Assume that for some dataset, we know the ground truth outcomes
$y\in\{0,1\}$. Note that these outcomes may differ statistically
between different populations, either because there are genuine
differences between the groups or because the model is somehow biased.
According to the situation, we may want our estimate $\hat{y}$ to take
account of these differences or to compensate for them.

Most definitions of fairness are based on \_ group fairness\_, which
deals with statistical fairness across the whole population.
Complementary to this is \_ individual fairness\_ which mandates that
similar individuals should be treated similarly regardless of group
membership. In this blog, we'll mainly focus on group fairness, three
definitions of which include: (i) demographic parity, (ii) equality of
odds, and (iii) equality of opportunity. We now discuss each in turn.

## Demographic Parity

\_ Demographic parity\_ or \_ statistical parity\_ suggests that a
predictor is unbiased if the prediction $\hat{y}$ is independent of the
protected attribute $p$ so that

\begin{equation}  
    Pr(\hat{y}|p) = Pr(\hat{y}). \tag{2.1}  
\end{equation}

Here, the same proportion of each population are classified as positive.
However, this may result in different false positive and true positive
rates if the true outcome $y$ does actually vary with the protected
attribute $p$.

Deviations from statistical parity are sometimes measured by the \_
statistical parity difference\_

\begin{equation}  
    \mbox{SPD} = Pr(\hat{y}=1, p=1) - Pr(\hat{y}=1, p=0), 
\tag{2.2}  
\end{equation}

or the \_ disparate impact\_ which replaces the difference in this
equation with a ratio. Both of these are measures of \_
discrimination\_ (i.e. deviation from fairness).

## Equality of odds

\_ Equality of odds\_ is satisfied if the prediction $\hat{y}$ is
conditionally independent to the protected attribute $p$, given the true
value $y$:

\begin{equation}  
    Pr(\hat{y}|y,p) = Pr(\hat{y}| y). \tag{2.3}  
\end{equation}

This means that the true positive rate and false positive rate will be
the same for each population; each error type is matched between each
group. 

## Equality of opportunity

\_ Equality of opportunity\_ has the same mathematical formulation as
equality of odds, but is focused on one particular label $y=1$ of the
true value so that:

\begin{equation}  
    Pr(\hat{y}|y=1,p) = Pr(\hat{y}| y=1).  \tag{2.4}  
\end{equation}

In this case, we want the true positive rate $Pr(\hat{y}=1|y=1)$ to be
the same for each population with no regard for the errors when $y=0$.
In effect it means that the same proportion of each population receive
the "good'' outcome $y=1$.

Deviation from equality of opportunity is measured by the \_ equal
opportunity difference\_:

\begin{equation}  
    \mbox{EOD} = Pr(\hat{y}=1,y=1, p=1) - Pr(\hat{y}=1,y=1, p=0). 
\tag{2.5}  
\end{equation}
 
## Worked example: loans

\#\# Worked example: loans
--------------------------

To make these ideas concrete, we consider the example of an algorithm
that predicts credit rating scores for loan decisions. This scenario
follows from the work of [Hardt \_ et al.\_ (2016)] and the associated
[blog]. 

There are two pools of loan applicants $p\in\{0,1\}$ that we'll
describe as the blue and yellow populations. We assume that we are given
historical data, so we know both the credit rating and whether the
applicant actually defaulted on the loan ($y=0$) or repaid it ($y=1$).

We can now think of four groups of data corresponding to (i) the blue
and yellow populations and (ii) whether they did or did not repay the
loan. For each of these four groups we have a distribution of credit
ratings (figure 1). In an ideal world, the two distributions for the
yellow population would be exactly the same as those for the blue
population. However, as figure 1 shows, this is clearly not the case
here.

<span class="caption">Figure 1. Loan example setup. We consider two
populations: blue (protected attribute $p=0$) and yellow (protected
attribute $p=1$) a) Empirically observed distributions of credit rating
scores produced by a machine learning algorithm for the blue population
for the cases where they either defaulted on the loan (dashed line) or
paid it back (solid line). b) Empirically observed distributions for the
yellow population. Note that the distributions differ in weighting, mean
position and overlap for the blue and yellow groups.</span>

Why might the distributions for blue and yellow populations be
different? It could be that the behaviour of the populations is
identical, but the credit rating algorithm is biased; it may favor one
population over another or simply be more noisy for one group.
Alternatively, it could be that that the populations genuinely behave
differently. In practice, the differences in blue and yellow
distributions are probably attributable to a combination of these
factors.

Let's assume that we can't retrain the credit score prediction
algorithm; our job is to adjudicate whether each individual is refused
the loan ($\hat{y}=0)$ or granted it ($\hat{y}=1$). Since we only have
the credit score $\mbox{f}\[\mathbf{x}$$\]$ to go on, the best we can
do is to assign different thresholds $\tau\_{0}$ and $\tau\_{1}$ for
the blue and yellow populations so that the loan is granted if
$f\[\mathbf{x}$$\]$ $&gt;\tau\_{0}$ for the blue population and
$f\[\mathbf{x}$$\]$ $&gt;\tau\_{1}$ for the yellow population. 

  [Hardt \_ et al.\_ (2016)]: https://arxiv.org/abs/1610.02413
  [blog]: http://research.google.com/bigpicture/attacking-discrimination-in-ml

## Comparison of fairness criteria

We'll now consider different possible ways to set these thresholds that
result in different senses of fairness. We emphasize that we are not
advocating any particular criterion, but merely exploring the
ramifications of different choices.

**\*Blindness to protected attribute:\*** We choose the same threshold
for blue and yellow populations. This sounds sensible, but it neither
guarantees that the overall frequency of loans, nor the frequency of
successful loans will be the same for the two groups. For the thresholds
chosen in figure 2a, many more loans are made to the yellow population
than the blue population (figure 2b). Moreover, examination of the
receiver operating characteristic (ROC) curve shows that both the rate
of true positives $Pr(\hat{y}=1|y=1)$ and false alarms
$Pr(\hat{y}=1|y=0)$ differ for the two groups (figure 2c). 

<span class="caption">Figure 2. Blindness to protected attribute. a) If
we ignore the protected attribute and set the thresholds (black solid
lines) to be the same for both blue and yellow populations, then
fairness is not achieved. b) The overall positive rate $Pr(\hat{y})$
(proportion of people given loans regardless of outcome) is not the same
in each population. c) ROC curves for the curves (points represent
chosen thresholds). The true positive rate $Pr(\hat{y}=1|y=1)$
(proportion of people who were offered loan and paid it back
successfully) is not the same for each population either as shown by the
different vertical positions of the points on the ROC curves.</span>

**\*Equality of odds:\*** This definition of fairness proposes that the
false positive and true positive rates should be the same for both
populations. This also sounds reasonable, but figure 2c shows that it is
not possible for this example. There is no combination of thresholds
that can achieve this because the ROC curves do not intersect. Even if
they did, we would be stuck giving loans based on the particular false
positive and true positive rates at the intersection which might not be
desirable.

**\*Demographic parity:\*** The threshold could be chosen so that the
same proportion of each group are classified as $\hat{y} =1$ and given
loans (figure 3). We make an equal number of loans to each group despite
the different tendencies of each to repay (figure 3b). This has the
disadvantage that the true positive and false positive rates might be
completely different in different populations (figure 3c). From the
perspective of the lender, it is desirable to give loans in proportion
to people's ability to pay them back. From the perspective of an
individual in a more reliable group, it may seem unfair that the other
group gets offered the same number of loans despite the fact they are
less reliable.

&gt; <span class="caption">Figure 3. Demographic parity. a) We carefully
choose different thresholds for each population </span>


## Trade-offs

We have seen that there is no straightforward way to choose thresholds on an existing classifier for different populations, so that all definitions of fairness are satisfied. Now we'll investigate a different approach that aims to make the classification performance more similar for the two models.

The ROC curves show that accuracy is higher in predicting whether the blue population will repay the loan as opposed to the yellow group (i.e. the blue ROC curve is everywhere higher than the yellow one). What if we try to reduce the accuracy for the blue population so that this more nearly matches? One way to do this is to add noise to the credit score for the blue population (figure 5). As we add increasing amounts of noise the blue ROC curve moves towards the positive diagonal and at some point will cross the yellow ROC curve. Now equality of odds can be achieved.

 Figure 5. Adding noise to improve fairness. a-c) We progressively add noise to the blue credit scores, causing the distributions to become flatter and overlap more. e) As we add noise, the blue ROC curve moves towards the positive diagonal and at some point must intersect the yellow ROC curve, meaning that equality of odds can be achieved.

Unfortunately, this approach has two unattractive features. First, we now make worse decisions for the blue population; it is a general feature of most remedial approaches that there is a trade off between accuracy and fairness (Kamiran & Calders 2012; Corbett-Davies _ et al._ 2017). Second, adding noise violates individual fairness. Two identical members of the blue population may have different noise values added to the scores, resulting in different decisions on their loans.

## Bias mitigation algorithms

The conclusion of the worked loan example is that it is very hard to remove bias once the classifier has already been trained, even for very simple cases. For further information, the reader is invited to consult Kamiran & Calders (2012), Hardt_ et al_. (2016), Menon & Williamson (2017) and Pleiss_ et al._ (2017).

Post-Processing	In-Processing	Pre-Processing	Data Collection
• Change thresholds
• Trade off accuracy for fairness	• Adversarial training
• Regularize for fairness
• Constrain to be fair	• Modify labels
• Modify input data
• Modify label/data pairs
• Weight label/data pairs	• Identify lack of examples
or variates and collect
Thankfully, there are approaches to deal with bias at all stages of the data collection, preprocessing, and training pipeline (figure 6). In this section we consider some of these methods. In the ensuing discussion, we'll assume that the true behaviour of the different populations is the same. Hence, we are interested in making sure that the predictions of our system do not differ for each population.

## Pre-processing approaches
A straightforward approach to eliminating bias from datasets would be to
remove the protected attribute and other elements of the data that are
suspected to contain related information. Unfortunately, such
suppression rarely suffices. There are often subtle correlations in the
data that mean that the protected attribute can be reconstructed. For
example, we might remove race, but retain information about the
subject's address, which could be strongly correlated with the race.

The degree to which there are dependencies between the data
$\mathbf{x}$ and the protected attribute $p$ can be measured using the
mutual information

\begin{equation}  
\mbox{LP} = \sum\_{\mathbf{x},p} Pr(\mathbf{x},p)
\log\left\[\frac{Pr(\mathbf{x},p)}{Pr(\mathbf{x})Pr(p)}\right\], 
\tag{2.6}  
\end{equation}

which is known as the \_ latent prejudice\_ ([Kamishima \_ et al.\_
2011]). As this measure increases, the protected attribute becomes more
predictable from the data. Indeed, [Feldman \_ et al. \_(2015)] and
[Menon & Williamson (2017)] have shown that the predictability of the
protected attribute puts mathematical bounds on the potential
discrimination of a classifier.

We'll now discuss four approaches for removing bias by manipulating the
dataset. Respectively, these modify the labels $y$, the observed data
$\mathbf{x}$, the data/label pairs $\{\mathbf{x},y\}$, and the
weighting of these pairs.

#### **\*Manipulating labels\***

[Kamiran & Calders (2012)] proposed changing some of the training labels
which they term \_ massaging\_ the data. They compute a classifier on
the original dataset and find examples close to the decision surface.
They then swap the labels in such a way that a positive outcome for the
disadvantaged group is more likely and re-train. This is a heuristic
approach that empirically improves fairness at the cost of accuracy.

#### **\*Manipulating observed data\***

[Feldman \_ et al.\_ (2015)][Feldman \_ et al. \_(2015)] proposed
manipulating individual data dimensions $x$ in a way that depends on the
protected attribute $p$. They align the cumulative distributions
$F\_{0}\[x\]$ and $F\_{1}\[x\]$ for feature $x$ when the protected
attribute $p$ is 0 and 1 respectively to a median cumulative
distribution $F\_{m}\[x\]$. This is similar to standardising test scores
across different high schools (figure 7) and is termed \_ disparate
impact removal\_. This approach has the disadvantage that it treats each
input variable $x\in\mathbf{x}$ separately and ignores their
interactions.

  [Kamishima \_ et al.\_ 2011]: https://ieeexplore.ieee.org/abstract/document/6137441
  [Feldman \_ et al. \_(2015)]: https://arxiv.org/abs/1412.3756
  [Menon & Williamson (2017)]: https://arxiv.org/abs/1705.09055
  [Kamiran & Calders (2012)]: https://dl.acm.org/citation.cfm?id=3225850

In the previous section, we introduced the latent prejudice measure
based on the mutual information between the data $\mathbf{x}$ and the
protected attribute $p$. Similarly, we can measure the dependence
between the labels $y$ and the protected attribute $p$:

\begin{equation}  
\mbox{IP} = \sum\_{y,p} Pr(y,p)
\log\left\[\frac{Pr(y,p)}{Pr(y)Pr(p)}\right\].  \tag{2.7}  
\end{equation}

This is known as the \_ indirect prejudice\_ ([Kamishima \_ et al.\_
2011]). Intuitively, if there is no way to predict the labels from the
protected attribute and vice-versa then there is no scope for bias. 

One approach to removing bias during training is to explicitly remove
this dependency using adversarial learning. Other approaches to removing
bias include penalizing the mutual information using regularization,
fitting the model under the constraint that it is not biased. We'll
briefly discuss each in turn.

#### **\*Adversarial de-biasing\***

Adversarial-debiasing ([Beutel \_ et al. \_2017]; [Zhang\_ et
al.\_ 2018]) reduces evidence of protected attributes in predictions by
trying to simultaneously fool a second classifier that tries to guess
the protected attribute $p$. [Beutel \_ et al.
\_(2017)][Beutel \_ et al. \_2017] force both classifiers to use a
shared representation and so minimizing the performance of the
adversarial classifier means removing all information about the
protected attribute from this representation (figure 8).

<span class="caption">Figure 8. Adversarial learning for fairness.
Beutel et al. (2017) learned a representation for classification that
was also used to predict the protected attribute. The system was trained
in an adversarial manner, encouraging good performance by the system but
punishing correct classification of the protected attribute. In this way
a representation that does not contain information about the protected
attribute is learned.</span>

[Zhang \_ et al. \_(2018)][Zhang\_ et al.\_ 2018] use the adversarial
component to predict $p$ from (i) the final classification logits
$f\[\mathbf{x}\]$ (to ensure demographic parity), (ii) the
classification logits $f\[\mathbf{x}\]$ and the true class $y$ (to
ensure equality of odds), or (iii) the final classification logits and
the true result for just one class (to ensure equality of opportunity). 

  [Kamishima \_ et al.\_ 2011]: https://ieeexplore.ieee.org/abstract/document/6137441
  [Beutel \_ et al. \_2017]: https://arxiv.org/abs/1707.00075
  [Zhang\_ et al.\_ 2018]: https://arxiv.org/abs/1801.07593
  
  [Kamishima \_ et al.\_ (2011)] proposed adding an extra regularization
condition to the output of logistic regression classifier that tried to
minimize the mutual information between the protected attribute and the
prediction $\hat{y}$. They first re-arranged the indirect prejudice
expression using the definition of conditional probability to get

 \begin{eqnarray}  
\mbox{PI} &=& \sum\_{y,p} Pr(y|\mathbf{x},p)
\log\left\[\frac{Pr(y,p)}{Pr(y)Pr(p)}\right\]\nonumber\\  
 &=& \sum\_{y,p} Pr(y|\mathbf{x},p)
\log\left\[\frac{Pr(y|p)}{Pr(y)}\right\].  \tag{2.8}  
 \end{eqnarray}

Then, they formulate a regularization loss based on the expectation of
this over the data set:

  \begin{equation}  
    \mbox{L}\_{reg} = \sum\_{i}\sum\_{\hat{y},p}
Pr(\hat{y}\_{i}|\mathbf{x}\_{i},p\_{i})\log\left\[\frac{Pr(\hat{y}\_{i}|p\_{i})}{Pr(\hat{y}\_{i})}\right\] 
\tag{2.9}  
 \end{equation}

where $i$ indexes the data examples, which they add to the main training
loss.

  [Kamishima \_ et al.\_ (2011)]: https://ieeexplore.ieee.org/abstract/document/6137441

## Prejudice removal by regularization

[Kamishima \_ et al.\_ (2011)] proposed adding an extra regularization
condition to the output of logistic regression classifier that tried to
minimize the mutual information between the protected attribute and the
prediction $\hat{y}$. They first re-arranged the indirect prejudice
expression using the definition of conditional probability to get

 \begin{eqnarray}  
\mbox{PI} &=& \sum\_{y,p} Pr(y|\mathbf{x},p)
\log\left\[\frac{Pr(y,p)}{Pr(y)Pr(p)}\right\]\nonumber\\  
 &=& \sum\_{y,p} Pr(y|\mathbf{x},p)
\log\left\[\frac{Pr(y|p)}{Pr(y)}\right\].  \tag{2.8}  
 \end{eqnarray}

Then, they formulate a regularization loss based on the expectation of
this over the data set:

  \begin{equation}  
    \mbox{L}\_{reg} = \sum\_{i}\sum\_{\hat{y},p}
Pr(\hat{y}\_{i}|\mathbf{x}\_{i},p\_{i})\log\left\[\frac{Pr(\hat{y}\_{i}|p\_{i})}{Pr(\hat{y}\_{i})}\right\] 
\tag{2.9}  
 \end{equation}

where $i$ indexes the data examples, which they add to the main training
loss.

  [Kamishima \_ et al.\_ (2011)]: https://ieeexplore.ieee.org/abstract/document/6137441

## Fairness constraints


[Zafar \_ et al.\_ (2015)] formulated unfairness in terms of the
covariance between the protected attribute $\{p\_{i}\}\_{i=1}^{I}$ and
the signed distances
$\{d\[\mathbf{x}\_{i},\boldsymbol\theta\]\}\_{i=1}^{I}$ of the
associated feature vectors $\{\mathbf{x}\_{i}\}\_{i=1}^{I}$ from the
decision boundary, where $\boldsymbol\theta$ denotes the model
parameters. Let $\overline{p}$ represent the mean value of the
protected attribute. They then minimize the main loss function such that
the covariance remains within some threshold $t$. 

\begin{equation}  
\begin{aligned}  
& \underset{\boldsymbol\theta}{\text{minimize}}  
& & L\[\boldsymbol\theta\] \\  
& \text{subject to}  
& &
\frac{1}{I}\sum\_{i=1}^{I}(p\_{i}-\overline{p})d\[\mathbf{x}\_{i},\boldsymbol\theta\]
\leq t\\   
& & &
\frac{1}{I}\sum\_{i=1}^{I}(p\_{i}-\overline{p})d\[\mathbf{x}\_{i},\boldsymbol\theta\]
\geq -t   
\end{aligned} \tag{2.10}  
\end{equation}

This constrained optimization problem can also be written as a
regularized optimization problem in which the fairness constraints are
moved to the objective and the corresponding Lagrange multipliers act as
regularizers. [Zafar \_ et al. \_(2015)][Zafar \_ et al.\_ (2015)] also
introduced a second formulation where they maximize fairness under
accuracy constraints.

  [Zafar \_ et al.\_ (2015)]: https://arxiv.org/abs/1507.05259
  
  

## Other approaches

 [Zemel \_ et al.\_ (2013)] presented a method that maps data to an
intermediate space in a way that depends on the protected attribute and
obfuscates information about that attribute. Since this mapping is
learnt during training, this method could considered either a
pre-processing approach or an in-processing algorithm.

[Chen \_ et al.\_ (2018)] argue that a trade off between fairness and
accuracy may not be acceptable and that these challenges should be
addressed through data collection. They aim to diagnose unfairness
induced by inadequate data and unmeasured predictive variables and
prescribe data collection approaches to remedy these problems.

## Summary and further resources

In this tutorial, we've discussed what it means for a classifier to be
fair, how to quantify the degree of bias in a dataset and methods to
remedy unfairness at all stages in the pipeline. An empirical analysis
of fairness-based interventions is presented in [Friedler \_ et al.\_
(2019)]. There are a large number of toolkits available to help evaluate
fairness, the most comprehensive of which is [AI Fairness 360].

This tutorial has been limited to a discussion of supervised learning
algorithms, but there is also an orthogonal literature on bias in NLP
embeddings (e.g. [Zhao \_ et al.\_ 2019]).

  [Zemel \_ et al.\_ (2013)]: https://dl.acm.org/citation.cfm?id=3042973
  [Chen \_ et al.\_ (2018)]: https://arxiv.org/abs/1805.12002
  [Friedler \_ et al.\_ (2019)]: https://arxiv.org/abs/1802.04422
  [AI Fairness 360]: https://aif360.mybluemix.net
  
## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r }

```





```





```

